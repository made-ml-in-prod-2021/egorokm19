# Homework 3

## Запуск airflow
Перед запуском необходимо зайти в файл: homework3/dags/utils.py и заменить путь в DEFAULT_PATH на свой!
После чего нужно выполнить команду:
```bash
docker-compose up --build
```

## Запуск тестов

```bash
docker exec -it airflow_ml_dags_scheduler_1
pytest -v .
```


Данный проект оцениваю на 38 баллов.

## Критерии

№ | Описание | Баллы
--- | --- | ---
0 | ~~Поднимите airflow локально, используя docker compose (можно использовать из примера https://github.com/made-ml-in-prod-2021/airflow-examples/)~~ | 0
1 | ~~Реализуйте dag, который генерирует данные для обучения модели (генерируйте данные, можете использовать как генератор синтетики из первой дз, так и что-то из датасетов sklearn), вам важно проэмулировать ситуации постоянно поступающих данных~~ | 5
2 | ~~Реализуйте dag, который обучает модель еженедельно, используя данные за текущий день. В вашем пайплайне должно быть как минимум 4 стадии, но дайте волю своей фантазии=)~~ | 10
3 | ~~Реализуйте dag, который использует модель ежедневно~~ | 5
3a | ~~Реализуйте сенсоры на то, что данные готовы для дагов тренировки и обучения~~ | 3
4 | ~~Поставить все необходимые пакеты в образ с airflow и использовать bash operator, python operator и 1 из дагов реализован с помощью DockerOperator и все даги реализованы только с помощью DockerOperator~~ | 15
5 | ~~Протестируйте ваши даги~~ | 5
6 | В docker compose так же настройте поднятие mlflow и запишите туда параметры обучения, метрики и артефакт(модель) | 5
7 | Вместо пути в airflow variables  используйте апи Mlflow Model Registry | 5
8 | ~~Настройте alert в случае падения дага~~ | 3
9 | ~~Tрадиционно, самооценка~~ | 1